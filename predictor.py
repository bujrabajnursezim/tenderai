"""–ì–æ—Ç–æ–≤—ã–π –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä ‚Äî –∏–º–ø–æ—Ä—Ç–∏—Ä—É–π –≤ Streamlit"""
import pickle, re, numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import MinMaxScaler
from extract_requirements import extract_requirements, REQUIREMENT_LABELS
from winner_history import check_winner_history
from legal_compliance import check_legal_compliance, get_legal_summary
embedder = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
with open("model.pkl", "rb") as f: model = pickle.load(f)
with open("scaler.pkl", "rb") as f: scaler = pickle.load(f)

def extract_features(text):
    brand_model_patterns = [
        r"(Dell|HP|Lenovo|Apple|Samsung|Philips|Siemens|Toyota|BMW|Mercedes)\s+\w+[\s\-]\w+",
        r"(—Å—Ç—Ä–æ–≥–æ|–∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ|—Ç–æ–ª—å–∫–æ)\s+[–ê-–ØA-Z]\w+",
        r"–∞—Ä—Ç–∏–∫—É–ª\s+[\w\-]+",
        r"—Å–µ—Ä–∏–π–Ω\w+\s+–Ω–æ–º–µ—Ä\s+[\w\-]+",
        r"–º–æ–¥–µ–ª—å\s+[A-Z\d][\w\-]+",
    ]
    brand_count = sum(len(re.findall(p, text, re.IGNORECASE)) for p in brand_model_patterns)
    brand_score = min(brand_count / 3, 1.0)

    restriction_words = ["—Å—Ç—Ä–æ–≥–æ","–∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ","—Ç–æ–ª—å–∫–æ","–∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–π –¥–∏–ª–µ—Ä","–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å","–∞–Ω–∞–ª–æ–≥–∏ –Ω–µ –ø—Ä–∏–Ω–∏–º–∞—é—Ç—Å—è"]
    restriction_score = min(sum(1 for w in restriction_words if w.lower() in text.lower()) / 3, 1.0)

    tight_deadline = bool(re.search(r"(–≤ —Ç–µ—á–µ–Ω–∏[–µ–∏]\s*[1-3]\s*(—Ä–∞–±–æ—á|–∫–∞–ª–µ–Ω–¥–∞—Ä)\w+|[1-3]\s*(—Ä–∞–±–æ—á–∏—Ö|–∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã—Ö|–∂“±–º—ã—Å|–∫“Ø–Ω—Ç—ñ–∑–±–µ–ª—ñ–∫)?\s*(–¥–Ω|–¥–µ–Ω—å|–∫“Ø–Ω)|–∑–∞\s*1\s*–¥–µ–Ω—å|1\s*—Ä–∞–±–æ—á–µ–≥–æ\s*–¥–Ω—è)", text, re.IGNORECASE))
    
    precise_patterns = [r"\d+[,.]?\d*\s*(–ì–ì—Ü|–ú–ì—Ü|–ì–ë|–ú–ë|GB|–¥—é–π–º|–º–º|–∫–≥)", r"–≤–µ—Ä—Å–∏—è\s+\d+\.\d+", r"\d{3,}[xX√ó]\d{3,}", r"–∞—Ä—Ç–∏–∫—É–ª\s+[\w\-]{4,}", r"[A-Z]{2,}\d{4,}[\w\-]*"]
    precise_score = min(sum(len(re.findall(p, text, re.IGNORECASE)) for p in precise_patterns) / 5, 1.0)

    supplier_patterns = [r"–¢–û–û\s+[\w\s]+", r"–ë–ò–ù\s+\d{12}", r"–ò–ü\s+[\w\s]+"]
    supplier_score = min(sum(len(re.findall(p, text, re.IGNORECASE)) for p in supplier_patterns) / 2, 1.0)

    return {"brand_model": brand_score, "restriction": restriction_score, "tight_deadline": 1.0 if tight_deadline else 0.0, "precise_params": precise_score, "supplier_lock": supplier_score}

def predict_single(text):
    vec = embedder.encode([text])
    raw = model.decision_function(vec)[0]
    anomaly = float(scaler.transform([[-raw]])[0][0])
    feats = extract_features(text)

    # –£–≤–µ–ª–∏—á–∏–ª–∏ –≤–µ—Å–∞ –±—Ä–µ–Ω–¥—ã + –æ–≥—Ä–∞–Ω–∏—á–∏—Ç–µ–ª–∏
    risk = round((
        0.10 * anomaly +
        0.35 * feats["brand_model"] +
        0.30 * feats["restriction"] +
        0.10 * feats["tight_deadline"] +
        0.05 * feats["precise_params"] +
        0.10 * feats["supplier_lock"]
    ) * 100, 1)

    if risk >= 70: level, color, rec = "üî¥ –í–´–°–û–ö–ò–ô", "red", "–¢—Ä–µ–±—É–µ—Ç—Å—è –¥–µ—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞. –í–æ–∑–º–æ–∂–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏."
    elif risk >= 40: level, color, rec = "üü° –°–†–ï–î–ù–ò–ô", "orange", "–û—Ç–¥–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç–∏. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞."
    else: level, color, rec = "üü¢ –ù–ò–ó–ö–ò–ô", "green", "–î–æ–∫—É–º–µ–Ω—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º."

    sentences = re.split(r"[.!?\n]", text)
    suspicious = [s.strip() for s in sentences if len(s.strip()) > 20 and (
        re.search(r"\d+[.,]\d+", s) or
        re.search(r"—Å—Ç—Ä–æ–≥–æ|–∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ|—Ç–æ–ª—å–∫–æ", s, re.IGNORECASE) or
        re.search(r"[A-Z]{2,}\d+", s)
    )][:5]

    result = {
        "risk_score": risk,
        "level": level,
        "color": color,
        "recommendation": rec,
        "components": {
            "–ê–Ω–æ–º–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞": round(anomaly*100,1),
            "–ë—Ä–µ–Ω–¥—ã –∏ –º–æ–¥–µ–ª–∏": round(feats["brand_model"]*100,1),
            "–°–ª–æ–≤–∞-–æ–≥—Ä–∞–Ω–∏—á–∏—Ç–µ–ª–∏": round(feats["restriction"]*100,1),
            "–ñ—ë—Å—Ç–∫–∏–µ —Å—Ä–æ–∫–∏": round(feats["tight_deadline"]*100,1),
            "–¢–æ—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã": round(feats["precise_params"]*100,1)
        },
        "suspicious_sentences": suspicious,
        "stats": {
            "total_chars": len(text),
            "precise_numbers": len(re.findall(r"\d+[.,]\d+", text)),
            "sentences": len(sentences)
        }
    }

    result['legal'] = check_legal_compliance(text)
    result['legal_summary'] = get_legal_summary(result['legal'])
    result['requirements'] = extract_requirements(text)
    result['requirement_labels'] = REQUIREMENT_LABELS
    result['winners'] = check_winner_history(text)
    return result